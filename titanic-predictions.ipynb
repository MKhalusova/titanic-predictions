{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dnn_utils_v2 import *\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "test_df_copy = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preview data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combine the data from train and test to not perform the same data clean up operations twice - once per data set\n",
    "targets = train_df.Survived\n",
    "train_df.drop('Survived', 1, inplace=True)\n",
    "combined_sets = train_df.append(test_df)\n",
    "combined_sets.reset_index(inplace=True)\n",
    "combined_sets.drop('index', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1309, 11)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_sets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropping PassengerId as it's just an index. \n",
    "combined_sets = combined_sets.drop(['PassengerId'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass         0\n",
       "Name           0\n",
       "Sex            0\n",
       "Age          263\n",
       "SibSp          0\n",
       "Parch          0\n",
       "Ticket         0\n",
       "Fare           1\n",
       "Cabin       1014\n",
       "Embarked       2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for missing values in the combined set. \n",
    "combined_sets.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dummy encoding Pclass as it is also categorical\n",
    "pclass_dummies = pd.get_dummies(combined_sets['Pclass'], prefix=\"Pclass\")\n",
    "combined_sets = pd.concat([combined_sets,pclass_dummies],axis=1)\n",
    "combined_sets.drop('Pclass',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dummy encoding Sex column\n",
    "gender_dummies = pd.get_dummies(combined_sets['Sex'],prefix='Sex')\n",
    "combined_sets = pd.concat([combined_sets,gender_dummies],axis=1)\n",
    "combined_sets.drop('Sex',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extracting a column with a title\n",
    "combined_sets['Title'] = combined_sets['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dummy encoding titles\n",
    "title_dummies = pd.get_dummies(combined_sets['Title'], prefix=\"Title\")\n",
    "combined_sets = pd.concat([combined_sets,title_dummies],axis=1)\n",
    "combined_sets.drop('Title',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extracting last name\n",
    "combined_sets['Last_Name'] = combined_sets['Name'].map(lambda name:name.split(',')[0])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we can drop Name column now\n",
    "combined_sets = combined_sets.drop(['Name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adding new feature - family size inlcuding the passenger\n",
    "combined_sets['FamilySize'] = combined_sets['SibSp'] + combined_sets['Parch'] +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting ticket_number\n",
    "combined_sets['Ticket_Number'] = combined_sets['Ticket'].map(lambda x:x.rsplit(' ', 1)[-1])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extracting ticket prefix\n",
    "def cleanTicket(ticket):\n",
    "        ticket = ticket.replace('.','')\n",
    "        ticket = ticket.replace('/','')\n",
    "        ticket = ticket.split()\n",
    "        if len(ticket) > 1:\n",
    "            return ticket[0]\n",
    "        else:\n",
    "            return 'XXX'\n",
    "    \n",
    "combined_sets['Ticket'] = combined_sets['Ticket'].map(cleanTicket)\n",
    "ticket_dummies = pd.get_dummies(combined_sets['Ticket'], prefix='Ticket')\n",
    "combined_sets = pd.concat([combined_sets, ticket_dummies], axis=1)\n",
    "combined_sets.drop('Ticket', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C    7\n",
       "S    5\n",
       "Name: Embarked, dtype: int64"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combined_sets[(combined_sets['Embarked'].isnull())]['Ticket_Number'] # 113572\n",
    "combined_sets[(combined_sets['Ticket_Number'].map(lambda x: x.startswith('1135')))]['Embarked'].value_counts()\n",
    "# judging by other people with similar ticket it's equally possible that they embarked in C or S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'C' value is more common for Embarked among people with a similar ticket number\n",
    "combined_sets['Embarked'].fillna('C', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now we can dummy encode 'Embarked'\n",
    "embarked_dummies = pd.get_dummies(combined_sets['Embarked'],prefix='Embarked')\n",
    "combined_sets = pd.concat([combined_sets,embarked_dummies],axis=1)\n",
    "combined_sets.drop('Embarked',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>...</th>\n",
       "      <th>Ticket_STONO</th>\n",
       "      <th>Ticket_STONO2</th>\n",
       "      <th>Ticket_STONOQ</th>\n",
       "      <th>Ticket_SWPP</th>\n",
       "      <th>Ticket_WC</th>\n",
       "      <th>Ticket_WEP</th>\n",
       "      <th>Ticket_XXX</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>60.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age  SibSp  Parch  Fare Cabin  Pclass_1  Pclass_2  Pclass_3  \\\n",
       "1043  60.5      0      0   NaN   NaN         0         0         1   \n",
       "\n",
       "      Sex_female  Sex_male     ...      Ticket_STONO  Ticket_STONO2  \\\n",
       "1043           0         1     ...                 0              0   \n",
       "\n",
       "      Ticket_STONOQ  Ticket_SWPP  Ticket_WC  Ticket_WEP  Ticket_XXX  \\\n",
       "1043              0            0          0           0           1   \n",
       "\n",
       "      Embarked_C  Embarked_Q  Embarked_S  \n",
       "1043           0           0           1  \n",
       "\n",
       "[1 rows x 70 columns]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learning more about the passenger with missing Fare\n",
    "combined_sets[combined_sets['Fare'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The passenger with missing fare has Pclass_3==1 Embarked_S== 1 FamilySize==1 Ticket_XXX==1 Title_Mr==1\n",
    "# Let's average fare among those with the same values\n",
    "m_fare = combined_sets[(combined_sets['Embarked_S']==1) & (combined_sets['Pclass_3']==1) & (combined_sets['FamilySize']==1) & (combined_sets['Ticket_XXX']==1) & (combined_sets['Title_Mr']==1) & (combined_sets['Age'] > 50)]['Fare'].mean()\n",
    "combined_sets['Fare'].fillna(m_fare, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dealing with NaN in Age\n",
    "def replace_age_with_mean(df, pclass, title): \n",
    "    mask = ((df[pclass]==1) & (df[title]==1))\n",
    "    med = df.loc[mask, 'Age'].mean()\n",
    "    df.loc[mask, 'Age'] = df.loc[mask, 'Age'].fillna(med)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = ['Title_Mrs','Title_Miss','Title_Mr','Title_Dr', 'Title_Master', 'Title_Ms']\n",
    "pclass = ['Pclass_1', 'Pclass_2', 'Pclass_3']\n",
    "for p in pclass:\n",
    "    for title in titles:\n",
    "        combined_sets = replace_age_with_mean(combined_sets, p, title) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# After this there's still one missing value for NaN because there was only one Ms in third class, we'll use the average among all Ms\n",
    "m = combined_sets.loc[(combined_sets['Title_Ms']==1), 'Age'].mean()\n",
    "combined_sets['Age'].fillna(m, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# time to deal with Cabin NaNs:\n",
    "# turns out here are the proportions\n",
    "# In third class there are 693 NaN out of 709\n",
    "# In second class - 254 out of 277\n",
    "# In first class - 67 out of 323\n",
    "# Intuition - the passengers in third and second class were not assigned cabins, and the ones in the first class are genuine missing values\n",
    "last_names = combined_sets[(combined_sets['Pclass_1']==1)]['Last_Name'].value_counts()\n",
    "# the list of last names that occur more than once. these might indicate a family together with the passenger class\n",
    "fams = last_names[last_names>1].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# looking for families by last name in the first class \n",
    "# list of families where some Cabin values are NaN\n",
    "fams_with_a_nan = [] \n",
    "# list of families where all Cabin values are NaN\n",
    "fams_with_all_nan = []\n",
    "for name in fams:\n",
    "    if combined_sets[(combined_sets['Pclass_1']==1) & (combined_sets['Last_Name']==name)]['Cabin'].isnull().all()==True: \n",
    "        fams_with_all_nan.append(name)\n",
    "    else:\n",
    "        if combined_sets[(combined_sets['Pclass_1']==1) & (combined_sets['Last_Name']==name)]['Cabin'].isnull().any()== True: \n",
    "            fams_with_a_nan.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# those whose family has a cabin, get assigned the same cabin\n",
    "for name in fams_with_a_nan:\n",
    "    mask = ((combined_sets['Pclass_1']==1) & (combined_sets['Last_Name']==name))\n",
    "    cabin = combined_sets[mask]['Cabin'].value_counts()\n",
    "    cabin = cabin.index.tolist()[0]\n",
    "    combined_sets.loc[mask, 'Cabin'] = combined_sets.loc[mask, 'Cabin'].fillna(cabin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now that we can't guess the exact cabin, let's replace the Cabin number with just the first letter \n",
    "mask = ((combined_sets['Cabin'].isnull()==False))\n",
    "combined_sets.loc[mask, 'Cabin'] = combined_sets.loc[mask, 'Cabin'].map(lambda c : c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the ones that don't have family members with a known cabin, we'll check what fare they have and compare to similar fares\n",
    "for name in fams_with_all_nan:\n",
    "    fare = combined_sets[(combined_sets['Pclass_1']==1) & (combined_sets['Last_Name']==name)]['Fare'].mean()\n",
    "    mask1 = ((combined_sets['Pclass_1']==1) & (combined_sets['Fare']>(fare-1)) & (combined_sets['Fare']<(fare+1)))\n",
    "    cabin = combined_sets[mask1]['Cabin'].value_counts()\n",
    "    cabin = cabin.index.tolist()\n",
    "    if not cabin: \n",
    "        cabin = np.nan\n",
    "    else: \n",
    "        cabin = cabin[0]\n",
    "    mask2 = ((combined_sets['Pclass_1']==1) & (combined_sets['Last_Name']==name))\n",
    "    combined_sets.loc[mask2, 'Cabin'] = combined_sets.loc[mask2, 'Cabin'].fillna(cabin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 52 NaN left in Cabin in first class. these are singles\n",
    "# get all list of fares for NaNs\n",
    "fares = combined_sets[(combined_sets['Pclass_1']==1) & (combined_sets['Cabin'].isnull())]['Fare'].value_counts()\n",
    "fares = fares.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for fare in fares:\n",
    "    mask1 = ((combined_sets['Pclass_1']==1) & (combined_sets['Fare']>(fare-1)) & (combined_sets['Fare']<(fare+1)))\n",
    "    cabin = combined_sets[mask1]['Cabin'].value_counts()\n",
    "    cabin = cabin.index.tolist()\n",
    "    if not cabin: \n",
    "        cabin = np.nan\n",
    "    else: \n",
    "        cabin = cabin[0]\n",
    "    mask2 = ((combined_sets['Pclass_1']==1) & (combined_sets['Fare']==fare))\n",
    "    combined_sets.loc[mask2, 'Cabin'] = combined_sets.loc[mask2, 'Cabin'].fillna(cabin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C    122\n",
       "B     74\n",
       "E     55\n",
       "D     46\n",
       "A     22\n",
       "T      1\n",
       "Name: Cabin, dtype: int64"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#still 3 left I'll assign the most popular value in the first class to them, which is C\n",
    "combined_sets[(combined_sets['Pclass_1']==1)]['Cabin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = ((combined_sets['Pclass_1']==1))\n",
    "combined_sets.loc[mask, 'Cabin'] = combined_sets.loc[mask, 'Cabin'].fillna('C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the rest Cabin values are from the second and third class, and they can be \"U\" for Unknown\n",
    "combined_sets['Cabin'].fillna('U', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now we can use dummy encoding again\n",
    "cabin_dummies = pd.get_dummies(combined_sets['Cabin'], prefix='Cabin')\n",
    "combined_sets = pd.concat([combined_sets,cabin_dummies], axis=1)\n",
    "combined_sets.drop('Cabin', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                   False\n",
       "SibSp                 False\n",
       "Parch                 False\n",
       "Fare                  False\n",
       "Pclass_1              False\n",
       "Pclass_2              False\n",
       "Pclass_3              False\n",
       "Sex_female            False\n",
       "Sex_male              False\n",
       "Title_Capt            False\n",
       "Title_Col             False\n",
       "Title_Don             False\n",
       "Title_Dona            False\n",
       "Title_Dr              False\n",
       "Title_Jonkheer        False\n",
       "Title_Lady            False\n",
       "Title_Major           False\n",
       "Title_Master          False\n",
       "Title_Miss            False\n",
       "Title_Mlle            False\n",
       "Title_Mme             False\n",
       "Title_Mr              False\n",
       "Title_Mrs             False\n",
       "Title_Ms              False\n",
       "Title_Rev             False\n",
       "Title_Sir             False\n",
       "Title_the Countess    False\n",
       "Last_Name             False\n",
       "FamilySize            False\n",
       "Ticket_Number         False\n",
       "                      ...  \n",
       "Ticket_SCA4           False\n",
       "Ticket_SCAH           False\n",
       "Ticket_SCOW           False\n",
       "Ticket_SCPARIS        False\n",
       "Ticket_SCParis        False\n",
       "Ticket_SOC            False\n",
       "Ticket_SOP            False\n",
       "Ticket_SOPP           False\n",
       "Ticket_SOTONO2        False\n",
       "Ticket_SOTONOQ        False\n",
       "Ticket_SP             False\n",
       "Ticket_STONO          False\n",
       "Ticket_STONO2         False\n",
       "Ticket_STONOQ         False\n",
       "Ticket_SWPP           False\n",
       "Ticket_WC             False\n",
       "Ticket_WEP            False\n",
       "Ticket_XXX            False\n",
       "Embarked_C            False\n",
       "Embarked_Q            False\n",
       "Embarked_S            False\n",
       "Cabin_A               False\n",
       "Cabin_B               False\n",
       "Cabin_C               False\n",
       "Cabin_D               False\n",
       "Cabin_E               False\n",
       "Cabin_F               False\n",
       "Cabin_G               False\n",
       "Cabin_T               False\n",
       "Cabin_U               False\n",
       "Length: 78, dtype: bool"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_sets.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probably can drop the Last Name now\n",
    "combined_sets = combined_sets.drop(['Last_Name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_sets = combined_sets.drop(['Ticket_Number'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# time to split it back to the train and test sets\n",
    "train_df = combined_sets.head(891)\n",
    "train_Y = targets\n",
    "test_df = combined_sets.iloc[891:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891,)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting the training data into a matrix\n",
    "train_array = train_df.as_matrix()\n",
    "Y_array = train_Y.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_array = Y_array.reshape(Y_array.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splitting the training data into training set and dev set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_set, dev_set, Y_train, Y_dev = train_test_split(train_array, Y_array)\n",
    "training_set = np.transpose(training_set)\n",
    "dev_set = np.transpose(dev_set)\n",
    "Y_train = np.transpose(Y_train)\n",
    "Y_dev = np.transpose(Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 223)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking shapes\n",
    "dev_set.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    The linear part of a layer's forward propagation.\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    Z = np.add(np.matmul(W, A), b)\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Forward propagation for the LINEAR->ACTIVATION layer\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2            \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)    \n",
    "    \n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"    \n",
    "    m = Y.shape[1]\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    \n",
    "    cost = np.squeeze(cost)      \n",
    "    assert(cost.shape == ())    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1.0/m) * np.matmul(dZ, A_prev.T)\n",
    "    db = (1.0/m) * np.sum(dZ, axis=-1, keepdims=True)\n",
    "    dA_prev = np.matmul(np.transpose(W), dZ)    \n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) \n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = 'sigmoid')\n",
    "    \n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\"+str(l+2)],current_cache,\"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp     \n",
    "                \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting the number of layers in NN and their sizes\n",
    "layers_dims = [training_set.shape[0], 20, 10, 10, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.004, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    costs = []                         # keep track of cost\n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Cost function\n",
    "        cost = compute_cost(AL, Y)\n",
    "            \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters. (without optimizations)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "                \n",
    "        # Print the cost every 1000 training example\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.716655\n",
      "Cost after iteration 1000: 0.586478\n",
      "Cost after iteration 2000: 0.539048\n",
      "Cost after iteration 3000: 0.487224\n",
      "Cost after iteration 4000: 0.460057\n",
      "Cost after iteration 5000: 0.444522\n",
      "Cost after iteration 6000: 0.433272\n",
      "Cost after iteration 7000: 0.428248\n",
      "Cost after iteration 8000: 0.422010\n",
      "Cost after iteration 9000: 0.416192\n",
      "Cost after iteration 10000: 0.411883\n",
      "Cost after iteration 11000: 0.407630\n",
      "Cost after iteration 12000: 0.403030\n",
      "Cost after iteration 13000: 0.400882\n",
      "Cost after iteration 14000: 0.397546\n",
      "Cost after iteration 15000: 0.396209\n",
      "Cost after iteration 16000: 0.392951\n",
      "Cost after iteration 17000: 0.390042\n",
      "Cost after iteration 18000: 0.388535\n",
      "Cost after iteration 19000: 0.386146\n",
      "Cost after iteration 20000: 0.383602\n",
      "Cost after iteration 21000: 0.381044\n",
      "Cost after iteration 22000: 0.378492\n",
      "Cost after iteration 23000: 0.377440\n",
      "Cost after iteration 24000: 0.375572\n",
      "Cost after iteration 25000: 0.373609\n",
      "Cost after iteration 26000: 0.371476\n",
      "Cost after iteration 27000: 0.370502\n",
      "Cost after iteration 28000: 0.368427\n",
      "Cost after iteration 29000: 0.367813\n",
      "Cost after iteration 30000: 0.364581\n",
      "Cost after iteration 31000: 0.363861\n",
      "Cost after iteration 32000: 0.363050\n",
      "Cost after iteration 33000: 0.359418\n",
      "Cost after iteration 34000: 0.360047\n",
      "Cost after iteration 35000: 0.358163\n",
      "Cost after iteration 36000: 0.356949\n",
      "Cost after iteration 37000: 0.351229\n",
      "Cost after iteration 38000: 0.355039\n",
      "Cost after iteration 39000: 0.352946\n",
      "Cost after iteration 40000: 0.348912\n",
      "Cost after iteration 41000: 0.345666\n",
      "Cost after iteration 42000: 0.346018\n",
      "Cost after iteration 43000: 0.343972\n",
      "Cost after iteration 44000: 0.345678\n",
      "Cost after iteration 45000: 0.344979\n",
      "Cost after iteration 46000: 0.342163\n",
      "Cost after iteration 47000: 0.341624\n",
      "Cost after iteration 48000: 0.340598\n",
      "Cost after iteration 49000: 0.337966\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCzMz65WThZmZ9crJwszMevX/A8y5XfM7T8vQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1cfdb5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(training_set, Y_train, layers_dims, num_iterations = 50000, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    A2, cache = L_model_forward(X, parameters)\n",
    "    predictions = np.array([0 if i <= 0.5 else 1 for i in np.squeeze(A2)])\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86%\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(parameters,training_set)\n",
    "print ('Accuracy: %d' % float((np.dot(Y_train,pred_train.T) + np.dot(1-Y_train,1-pred_train.T))/float(Y_train.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81%\n"
     ]
    }
   ],
   "source": [
    "pred_dev = predict(parameters, dev_set)\n",
    "print ('Accuracy: %d' % float((np.dot(Y_dev,pred_dev.T) + np.dot(1-Y_dev,1-pred_dev.T))/float(Y_dev.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_array = test_df.as_matrix()\n",
    "test_array = np.transpose(test_array)\n",
    "Y_test_predictions = predict(parameters, test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test_predictions = Y_test_predictions.astype(int)\n",
    "Y_test_df = pd.DataFrame(np.transpose(Y_test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df_copy['Survived'] = Y_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_copy = test_df_copy.drop(['Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare', 'Cabin','Embarked'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         0\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         1"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df_copy.to_csv('predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
