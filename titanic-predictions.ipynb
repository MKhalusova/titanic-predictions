{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dnn_utils_v2 import *\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "test_df_copy = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "5            6         0       3   \n",
       "6            7         0       1   \n",
       "7            8         0       3   \n",
       "8            9         1       3   \n",
       "9           10         1       2   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "5                                   Moran, Mr. James    male   NaN      0   \n",
       "6                            McCarthy, Mr. Timothy J    male  54.0      0   \n",
       "7                     Palsson, Master. Gosta Leonard    male   2.0      3   \n",
       "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
       "9                Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  \n",
       "5      0            330877   8.4583   NaN        Q  \n",
       "6      0             17463  51.8625   E46        S  \n",
       "7      1            349909  21.0750   NaN        S  \n",
       "8      2            347742  11.1333   NaN        S  \n",
       "9      0            237736  30.0708   NaN        C  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preview data\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combine the data from train and test to not perform the same data clean up operations twice - once per data set\n",
    "targets = train_df.Survived\n",
    "train_df.drop('Survived', 1, inplace=True)\n",
    "# merging train data and test data for future feature engineering\n",
    "combined_sets = train_df.append(test_df)\n",
    "combined_sets.reset_index(inplace=True)\n",
    "combined_sets.drop('index', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1309, 11)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_sets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropping PassengerId as it's just an index. \n",
    "combined_sets = combined_sets.drop(['PassengerId'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass         0\n",
       "Name           0\n",
       "Sex            0\n",
       "Age          263\n",
       "SibSp          0\n",
       "Parch          0\n",
       "Ticket         0\n",
       "Fare           1\n",
       "Cabin       1014\n",
       "Embarked       2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for missing values in the combined set. \n",
    "combined_sets.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's dummy encode Pclass as it is also categorical\n",
    "pclass_dummies = pd.get_dummies(combined_sets['Pclass'], prefix=\"Pclass\")\n",
    "combined_sets = pd.concat([combined_sets,pclass_dummies],axis=1)\n",
    "combined_sets.drop('Pclass',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dummy encoding for Sex\n",
    "gender_dummies = pd.get_dummies(combined_sets['Sex'],prefix='Sex')\n",
    "combined_sets = pd.concat([combined_sets,gender_dummies],axis=1)\n",
    "combined_sets.drop('Sex',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extracting column with a title\n",
    "combined_sets['Title'] = combined_sets['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dummy encoding titles\n",
    "title_dummies = pd.get_dummies(combined_sets['Title'], prefix=\"Title\")\n",
    "combined_sets = pd.concat([combined_sets,title_dummies],axis=1)\n",
    "combined_sets.drop('Title',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extracting last name\n",
    "combined_sets['Last_Name'] = combined_sets['Name'].map(lambda name:name.split(',')[0])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we can drop Name column now\n",
    "combined_sets = combined_sets.drop(['Name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adding new feature - family size inlcuding the passanger\n",
    "combined_sets['FamilySize'] = combined_sets['SibSp'] + combined_sets['Parch'] +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting ticket_number\n",
    "combined_sets['Ticket_Number'] = combined_sets['Ticket'].map(lambda x:x.rsplit(' ', 1)[-1])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extracting ticket prefix\n",
    "def cleanTicket(ticket):\n",
    "        ticket = ticket.replace('.','')\n",
    "        ticket = ticket.replace('/','')\n",
    "        ticket = ticket.split()\n",
    "        if len(ticket) > 1:\n",
    "            return ticket[0]\n",
    "        else:\n",
    "            return 'XXX'\n",
    "    \n",
    "combined_sets['Ticket'] = combined_sets['Ticket'].map(cleanTicket)\n",
    "ticket_dummies = pd.get_dummies(combined_sets['Ticket'], prefix='Ticket')\n",
    "combined_sets = pd.concat([combined_sets, ticket_dummies], axis=1)\n",
    "combined_sets.drop('Ticket', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>...</th>\n",
       "      <th>Ticket_SOTONO2</th>\n",
       "      <th>Ticket_SOTONOQ</th>\n",
       "      <th>Ticket_SP</th>\n",
       "      <th>Ticket_STONO</th>\n",
       "      <th>Ticket_STONO2</th>\n",
       "      <th>Ticket_STONOQ</th>\n",
       "      <th>Ticket_SWPP</th>\n",
       "      <th>Ticket_WC</th>\n",
       "      <th>Ticket_WEP</th>\n",
       "      <th>Ticket_XXX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>61.9792</td>\n",
       "      <td>B30</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80.0000</td>\n",
       "      <td>B28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>55.0000</td>\n",
       "      <td>E33</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>C87</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0000</td>\n",
       "      <td>C128</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>55.0000</td>\n",
       "      <td>E33</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>211.5000</td>\n",
       "      <td>C82</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>D6</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80.0000</td>\n",
       "      <td>B28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>61.9792</td>\n",
       "      <td>B36</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211.5000</td>\n",
       "      <td>C130</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>32.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211.5000</td>\n",
       "      <td>C132</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>211.5000</td>\n",
       "      <td>C80</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>211.5000</td>\n",
       "      <td>C80</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age  SibSp  Parch      Fare Cabin Embarked  Pclass_1  Pclass_2  \\\n",
       "54    65.0      0      1   61.9792   B30        C         1         0   \n",
       "61    38.0      0      0   80.0000   B28      NaN         1         0   \n",
       "166    NaN      0      1   55.0000   E33        S         1         0   \n",
       "252   62.0      0      0   26.5500   C87        S         1         0   \n",
       "351    NaN      0      0   35.0000  C128        S         1         0   \n",
       "356   22.0      0      1   55.0000   E33        S         1         0   \n",
       "377   27.0      0      2  211.5000   C82        C         1         0   \n",
       "782   29.0      0      0   30.0000    D6        S         1         0   \n",
       "829   62.0      0      0   80.0000   B28      NaN         1         0   \n",
       "917   22.0      0      1   61.9792   B36        C         1         0   \n",
       "965   35.0      0      0  211.5000  C130        C         1         0   \n",
       "966   32.5      0      0  211.5000  C132        C         1         0   \n",
       "1109  50.0      1      1  211.5000   C80        C         1         0   \n",
       "1298  50.0      1      1  211.5000   C80        C         1         0   \n",
       "\n",
       "      Pclass_3  Sex_female     ...      Ticket_SOTONO2  Ticket_SOTONOQ  \\\n",
       "54           0           0     ...                   0               0   \n",
       "61           0           1     ...                   0               0   \n",
       "166          0           1     ...                   0               0   \n",
       "252          0           0     ...                   0               0   \n",
       "351          0           0     ...                   0               0   \n",
       "356          0           1     ...                   0               0   \n",
       "377          0           0     ...                   0               0   \n",
       "782          0           0     ...                   0               0   \n",
       "829          0           1     ...                   0               0   \n",
       "917          0           1     ...                   0               0   \n",
       "965          0           1     ...                   0               0   \n",
       "966          0           0     ...                   0               0   \n",
       "1109         0           1     ...                   0               0   \n",
       "1298         0           0     ...                   0               0   \n",
       "\n",
       "      Ticket_SP  Ticket_STONO  Ticket_STONO2  Ticket_STONOQ  Ticket_SWPP  \\\n",
       "54            0             0              0              0            0   \n",
       "61            0             0              0              0            0   \n",
       "166           0             0              0              0            0   \n",
       "252           0             0              0              0            0   \n",
       "351           0             0              0              0            0   \n",
       "356           0             0              0              0            0   \n",
       "377           0             0              0              0            0   \n",
       "782           0             0              0              0            0   \n",
       "829           0             0              0              0            0   \n",
       "917           0             0              0              0            0   \n",
       "965           0             0              0              0            0   \n",
       "966           0             0              0              0            0   \n",
       "1109          0             0              0              0            0   \n",
       "1298          0             0              0              0            0   \n",
       "\n",
       "      Ticket_WC  Ticket_WEP  Ticket_XXX  \n",
       "54            0           0           1  \n",
       "61            0           0           1  \n",
       "166           0           0           1  \n",
       "252           0           0           1  \n",
       "351           0           0           1  \n",
       "356           0           0           1  \n",
       "377           0           0           1  \n",
       "782           0           0           1  \n",
       "829           0           0           1  \n",
       "917           0           0           1  \n",
       "965           0           0           1  \n",
       "966           0           0           1  \n",
       "1109          0           0           1  \n",
       "1298          0           0           1  \n",
       "\n",
       "[14 rows x 68 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combined_sets[(combined_sets['Embarked'].isnull())]['Ticket_Number'] # 113572\n",
    "combined_sets[(combined_sets['Ticket_Number'].map(lambda x: x.startswith('1135')))]\n",
    "# judging by other people with similar ticket it's equally possible that they embarked in C or S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'S' value is way more common than the rest, so I've decided to replace the 2 missing values with \"S\"\n",
    "combined_sets['Embarked'].fillna('C', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now that we have all the values, we can use dummy encoding to turn categorical values into numbers\n",
    "embarked_dummies = pd.get_dummies(combined_sets['Embarked'],prefix='Embarked')\n",
    "combined_sets = pd.concat([combined_sets,embarked_dummies],axis=1)\n",
    "combined_sets.drop('Embarked',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>...</th>\n",
       "      <th>Ticket_STONO</th>\n",
       "      <th>Ticket_STONO2</th>\n",
       "      <th>Ticket_STONOQ</th>\n",
       "      <th>Ticket_SWPP</th>\n",
       "      <th>Ticket_WC</th>\n",
       "      <th>Ticket_WEP</th>\n",
       "      <th>Ticket_XXX</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>60.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age  SibSp  Parch  Fare Cabin  Pclass_1  Pclass_2  Pclass_3  \\\n",
       "1043  60.5      0      0   NaN   NaN         0         0         1   \n",
       "\n",
       "      Sex_female  Sex_male     ...      Ticket_STONO  Ticket_STONO2  \\\n",
       "1043           0         1     ...                 0              0   \n",
       "\n",
       "      Ticket_STONOQ  Ticket_SWPP  Ticket_WC  Ticket_WEP  Ticket_XXX  \\\n",
       "1043              0            0          0           0           1   \n",
       "\n",
       "      Embarked_C  Embarked_Q  Embarked_S  \n",
       "1043           0           0           1  \n",
       "\n",
       "[1 rows x 70 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_sets[combined_sets['Fare'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_sets[(combined_sets['Fare'].isnull())]['Age'] #3701 Pclass_3 Embarked_S = 1 FamilySize = 1 Ticket_XXX =1 Title_Mr\n",
    "m_fare = combined_sets[(combined_sets['Embarked_S']==1) & (combined_sets['Pclass_3']==1) & (combined_sets['FamilySize']==1) & (combined_sets['Ticket_XXX']==1) & (combined_sets['Title_Mr']==1) & (combined_sets['Age'] > 50)]['Fare'].mean()\n",
    "combined_sets['Fare'].fillna(m_fare, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dealing with NaN in Age\n",
    "def replace_age_with_mean(df, pclass, title): \n",
    "    mask = ((df[pclass]==1) & (df[title]==1))\n",
    "    med = df.loc[mask, 'Age'].mean()\n",
    "    df.loc[mask, 'Age'] = df.loc[mask, 'Age'].fillna(med)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = ['Title_Mrs','Title_Miss','Title_Mr','Title_Dr', 'Title_Master', 'Title_Ms']\n",
    "pclass = ['Pclass_1', 'Pclass_2', 'Pclass_3']\n",
    "for p in pclass:\n",
    "    for title in titles:\n",
    "        combined_sets = replace_age_with_mean(combined_sets, p, title) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# After this there's still one missing value for NaN because there was only one Ms in third class, we'll use the average among all Ms\n",
    "m = combined_sets.loc[(combined_sets['Title_Ms']==1), 'Age'].mean()\n",
    "combined_sets['Age'].fillna(m, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# time to deal with Cabin NaNs:\n",
    "# turns out here are the proportions\n",
    "# In third class there are 693 NaN out of 709\n",
    "# In second class - 254 out of 277\n",
    "# In first class - 67 out of 323\n",
    "last_names = combined_sets[(combined_sets['Pclass_1']==1)]['Last_Name'].value_counts()\n",
    "# the list of last names that occur more than once. these might indicate a family together with the passenger class\n",
    "fams = last_names[last_names>1].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of families where some Cabin values are NaN, and some not\n",
    "fams_with_a_nan = [] \n",
    "# list of families where all Cabin values are NaN\n",
    "fams_with_all_nan = []\n",
    "for name in fams:\n",
    "    if combined_sets[(combined_sets['Pclass_1']==1) & (combined_sets['Last_Name']==name)]['Cabin'].isnull().all()==True: \n",
    "        fams_with_all_nan.append(name)\n",
    "    else:\n",
    "        if combined_sets[(combined_sets['Pclass_1']==1) & (combined_sets['Last_Name']==name)]['Cabin'].isnull().any()== True: \n",
    "            fams_with_a_nan.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's those whose family has a cabin, get assigned the same cabin\n",
    "for name in fams_with_a_nan:\n",
    "    mask = ((combined_sets['Pclass_1']==1) & (combined_sets['Last_Name']==name))\n",
    "    cabin = combined_sets[mask]['Cabin'].value_counts()\n",
    "    cabin = cabin.index.tolist()[0]\n",
    "    combined_sets.loc[mask, 'Cabin'] = combined_sets.loc[mask, 'Cabin'].fillna(cabin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now that we can't guess the exact cabin, let's replace the Cabin number with just the first letter \n",
    "mask = ((combined_sets['Cabin'].isnull()==False))\n",
    "combined_sets.loc[mask, 'Cabin'] = combined_sets.loc[mask, 'Cabin'].map(lambda c : c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in fams_with_all_nan:\n",
    "    fare = combined_sets[(combined_sets['Pclass_1']==1) & (combined_sets['Last_Name']==name)]['Fare'].mean()\n",
    "    mask1 = ((combined_sets['Pclass_1']==1) & (combined_sets['Fare']>(fare-1)) & (combined_sets['Fare']<(fare+1)))\n",
    "    cabin = combined_sets[mask1]['Cabin'].value_counts()\n",
    "    cabin = cabin.index.tolist()\n",
    "    if not cabin: \n",
    "        cabin = np.nan\n",
    "    else: \n",
    "        cabin = cabin[0]\n",
    "    mask2 = ((combined_sets['Pclass_1']==1) & (combined_sets['Last_Name']==name))\n",
    "    combined_sets.loc[mask2, 'Cabin'] = combined_sets.loc[mask2, 'Cabin'].fillna(cabin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 52 NaN left in Cabin in first class\n",
    "# get all list of fares for NaNs\n",
    "fares = combined_sets[(combined_sets['Pclass_1']==1) & (combined_sets['Cabin'].isnull())]['Fare'].value_counts()\n",
    "fares = fares.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for fare in fares:\n",
    "    mask1 = ((combined_sets['Pclass_1']==1) & (combined_sets['Fare']>(fare-1)) & (combined_sets['Fare']<(fare+1)))\n",
    "    cabin = combined_sets[mask1]['Cabin'].value_counts()\n",
    "    cabin = cabin.index.tolist()\n",
    "    if not cabin: \n",
    "        cabin = np.nan\n",
    "    else: \n",
    "        cabin = cabin[0]\n",
    "    mask2 = ((combined_sets['Pclass_1']==1) & (combined_sets['Fare']==fare))\n",
    "    combined_sets.loc[mask2, 'Cabin'] = combined_sets.loc[mask2, 'Cabin'].fillna(cabin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C    122\n",
       "B     74\n",
       "E     55\n",
       "D     46\n",
       "A     22\n",
       "T      1\n",
       "Name: Cabin, dtype: int64"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#still 3 left I'll assign the most popular value in the first class to them, which is C\n",
    "combined_sets[(combined_sets['Pclass_1']==1)]['Cabin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = ((combined_sets['Pclass_1']==1))\n",
    "combined_sets.loc[mask, 'Cabin'] = combined_sets.loc[mask, 'Cabin'].fillna('C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the rest Cabin values are from the second and third class, and they can be \"U\" for Unknown\n",
    "combined_sets['Cabin'].fillna('U', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now we can use dummy encoding again\n",
    "cabin_dummies = pd.get_dummies(combined_sets['Cabin'], prefix='Cabin')\n",
    "combined_sets = pd.concat([combined_sets,cabin_dummies], axis=1)\n",
    "combined_sets.drop('Cabin', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                   False\n",
       "SibSp                 False\n",
       "Parch                 False\n",
       "Fare                  False\n",
       "Pclass_1              False\n",
       "Pclass_2              False\n",
       "Pclass_3              False\n",
       "Sex_female            False\n",
       "Sex_male              False\n",
       "Title_Capt            False\n",
       "Title_Col             False\n",
       "Title_Don             False\n",
       "Title_Dona            False\n",
       "Title_Dr              False\n",
       "Title_Jonkheer        False\n",
       "Title_Lady            False\n",
       "Title_Major           False\n",
       "Title_Master          False\n",
       "Title_Miss            False\n",
       "Title_Mlle            False\n",
       "Title_Mme             False\n",
       "Title_Mr              False\n",
       "Title_Mrs             False\n",
       "Title_Ms              False\n",
       "Title_Rev             False\n",
       "Title_Sir             False\n",
       "Title_the Countess    False\n",
       "Last_Name             False\n",
       "FamilySize            False\n",
       "Ticket_Number         False\n",
       "                      ...  \n",
       "Ticket_SCA4           False\n",
       "Ticket_SCAH           False\n",
       "Ticket_SCOW           False\n",
       "Ticket_SCPARIS        False\n",
       "Ticket_SCParis        False\n",
       "Ticket_SOC            False\n",
       "Ticket_SOP            False\n",
       "Ticket_SOPP           False\n",
       "Ticket_SOTONO2        False\n",
       "Ticket_SOTONOQ        False\n",
       "Ticket_SP             False\n",
       "Ticket_STONO          False\n",
       "Ticket_STONO2         False\n",
       "Ticket_STONOQ         False\n",
       "Ticket_SWPP           False\n",
       "Ticket_WC             False\n",
       "Ticket_WEP            False\n",
       "Ticket_XXX            False\n",
       "Embarked_C            False\n",
       "Embarked_Q            False\n",
       "Embarked_S            False\n",
       "Cabin_A               False\n",
       "Cabin_B               False\n",
       "Cabin_C               False\n",
       "Cabin_D               False\n",
       "Cabin_E               False\n",
       "Cabin_F               False\n",
       "Cabin_G               False\n",
       "Cabin_T               False\n",
       "Cabin_U               False\n",
       "Length: 78, dtype: bool"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_sets.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probably can drop the Last Name now\n",
    "combined_sets = combined_sets.drop(['Last_Name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_sets = combined_sets.drop(['Ticket_Number'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Fare',\n",
       " 'Pclass_1',\n",
       " 'Pclass_2',\n",
       " 'Pclass_3',\n",
       " 'Sex_female',\n",
       " 'Sex_male',\n",
       " 'Title_Capt',\n",
       " 'Title_Col',\n",
       " 'Title_Don',\n",
       " 'Title_Dona',\n",
       " 'Title_Dr',\n",
       " 'Title_Jonkheer',\n",
       " 'Title_Lady',\n",
       " 'Title_Major',\n",
       " 'Title_Master',\n",
       " 'Title_Miss',\n",
       " 'Title_Mlle',\n",
       " 'Title_Mme',\n",
       " 'Title_Mr',\n",
       " 'Title_Mrs',\n",
       " 'Title_Ms',\n",
       " 'Title_Rev',\n",
       " 'Title_Sir',\n",
       " 'Title_the Countess',\n",
       " 'FamilySize',\n",
       " 'Ticket_A',\n",
       " 'Ticket_A4',\n",
       " 'Ticket_A5',\n",
       " 'Ticket_AQ3',\n",
       " 'Ticket_AQ4',\n",
       " 'Ticket_AS',\n",
       " 'Ticket_C',\n",
       " 'Ticket_CA',\n",
       " 'Ticket_CASOTON',\n",
       " 'Ticket_FC',\n",
       " 'Ticket_FCC',\n",
       " 'Ticket_Fa',\n",
       " 'Ticket_LP',\n",
       " 'Ticket_PC',\n",
       " 'Ticket_PP',\n",
       " 'Ticket_PPP',\n",
       " 'Ticket_SC',\n",
       " 'Ticket_SCA3',\n",
       " 'Ticket_SCA4',\n",
       " 'Ticket_SCAH',\n",
       " 'Ticket_SCOW',\n",
       " 'Ticket_SCPARIS',\n",
       " 'Ticket_SCParis',\n",
       " 'Ticket_SOC',\n",
       " 'Ticket_SOP',\n",
       " 'Ticket_SOPP',\n",
       " 'Ticket_SOTONO2',\n",
       " 'Ticket_SOTONOQ',\n",
       " 'Ticket_SP',\n",
       " 'Ticket_STONO',\n",
       " 'Ticket_STONO2',\n",
       " 'Ticket_STONOQ',\n",
       " 'Ticket_SWPP',\n",
       " 'Ticket_WC',\n",
       " 'Ticket_WEP',\n",
       " 'Ticket_XXX',\n",
       " 'Embarked_C',\n",
       " 'Embarked_Q',\n",
       " 'Embarked_S',\n",
       " 'Cabin_A',\n",
       " 'Cabin_B',\n",
       " 'Cabin_C',\n",
       " 'Cabin_D',\n",
       " 'Cabin_E',\n",
       " 'Cabin_F',\n",
       " 'Cabin_G',\n",
       " 'Cabin_T',\n",
       " 'Cabin_U']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(combined_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# time to split it back to the train and test sets\n",
    "train_df = combined_sets.head(891)\n",
    "train_Y = targets\n",
    "test_df = combined_sets.iloc[891:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891,)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting the training data into a matrix\n",
    "train_array = train_df.as_matrix()\n",
    "Y_array = train_Y.as_matrix()\n",
    "# train_array = np.transpose(train_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_array = Y_array.reshape(Y_array.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splitting the training data into training set and dev set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_set, dev_set, Y_train, Y_dev = train_test_split(train_array, Y_array)\n",
    "training_set = np.transpose(training_set)\n",
    "dev_set = np.transpose(dev_set)\n",
    "Y_train = np.transpose(Y_train)\n",
    "Y_dev = np.transpose(Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 223)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking shapes\n",
    "dev_set.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this point we have: \n",
    "# training_set: training examples in columns, size N_features x 668\n",
    "# Y_train: ground truth for the training_set, size 1 x 668\n",
    "# dev_set: dev set with examples in columns, size N_features x 223\n",
    "# Y_dev: ground truth for dev_set, size 1 x 223\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    The linear part of a layer's forward propagation.\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "#     Z = np.dot(W,A) + b    \n",
    "    Z = np.add(np.matmul(W, A), b)\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Forward propagation for the LINEAR->ACTIVATION layer\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)    \n",
    "    \n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "#     cost = - np.sum(np.multiply(np.log(AL),Y) + np.multiply(np.log(1-AL),1-Y))/m\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "#     dW = 1/m*np.dot(dZ,A_prev.T)\n",
    "#     db = 1/m*np.sum(dZ, axis=1,keepdims=True)\n",
    "#     dA_prev = np.dot(W.T,dZ)\n",
    "    dW = (1.0/m) * np.matmul(dZ, A_prev.T)\n",
    "    db = (1.0/m) * np.sum(dZ, axis=-1, keepdims=True)\n",
    "    dA_prev = np.matmul(np.transpose(W), dZ)    \n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = 'sigmoid')\n",
    "    \n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\"+str(l+2)],current_cache,\"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp     \n",
    "                \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "layers_dims = [training_set.shape[0], 20, 10, 10, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.004, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    costs = []                         # keep track of cost\n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Cost function\n",
    "        cost = compute_cost(AL, Y)\n",
    "            \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters. (without optimizations)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "                \n",
    "        # Print the cost every 1000 training example\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.325923\n",
      "Cost after iteration 1000: 0.501499\n",
      "Cost after iteration 2000: 0.477980\n",
      "Cost after iteration 3000: 0.464656\n",
      "Cost after iteration 4000: 0.455287\n",
      "Cost after iteration 5000: 0.440293\n",
      "Cost after iteration 6000: 0.428288\n",
      "Cost after iteration 7000: 0.421633\n",
      "Cost after iteration 8000: 0.427731\n",
      "Cost after iteration 9000: 0.409988\n",
      "Cost after iteration 10000: 0.424086\n",
      "Cost after iteration 11000: 0.423458\n",
      "Cost after iteration 12000: 0.402796\n",
      "Cost after iteration 13000: 0.414825\n",
      "Cost after iteration 14000: 0.395923\n",
      "Cost after iteration 15000: 0.388688\n",
      "Cost after iteration 16000: 0.394464\n",
      "Cost after iteration 17000: 0.388616\n",
      "Cost after iteration 18000: 0.388998\n",
      "Cost after iteration 19000: 0.386453\n",
      "Cost after iteration 20000: 0.383683\n",
      "Cost after iteration 21000: 0.377200\n",
      "Cost after iteration 22000: 0.389538\n",
      "Cost after iteration 23000: 0.388631\n",
      "Cost after iteration 24000: 0.372758\n",
      "Cost after iteration 25000: 0.384236\n",
      "Cost after iteration 26000: 0.381599\n",
      "Cost after iteration 27000: 0.373793\n",
      "Cost after iteration 28000: 0.379142\n",
      "Cost after iteration 29000: 0.373545\n",
      "Cost after iteration 30000: 0.374015\n",
      "Cost after iteration 31000: 0.367036\n",
      "Cost after iteration 32000: 0.368767\n",
      "Cost after iteration 33000: 0.368701\n",
      "Cost after iteration 34000: 0.383085\n",
      "Cost after iteration 35000: 0.366933\n",
      "Cost after iteration 36000: 0.353476\n",
      "Cost after iteration 37000: 0.369340\n",
      "Cost after iteration 38000: 0.363225\n",
      "Cost after iteration 39000: 0.365276\n",
      "Cost after iteration 40000: 0.361015\n",
      "Cost after iteration 41000: 0.354796\n",
      "Cost after iteration 42000: 0.354158\n",
      "Cost after iteration 43000: 0.355626\n",
      "Cost after iteration 44000: 0.360091\n",
      "Cost after iteration 45000: 0.361916\n",
      "Cost after iteration 46000: 0.347521\n",
      "Cost after iteration 47000: 0.351056\n",
      "Cost after iteration 48000: 0.369547\n",
      "Cost after iteration 49000: 0.335949\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmYXFWd//H3t6qruqvTna3TWejsISGJYQlEFpFhd4DB\nBUSFQVxGRRydcf3NoOMDjA6OM467MoqK4CgCboiAIiAIyGISICELWcjaWTtbL+n0/v39cW8Xle6q\n7g7p6uru+3k9z3266tate8/pdOpT555zzzV3R0REBCBW6AKIiMjgoVAQEZE0hYKIiKQpFEREJE2h\nICIiaQoFERFJUyjIsGRmvzez9xa6HCJDjUJB+pWZbTKzCwpdDne/2N3vKHQ5AMzscTP74AAcp9jM\nbjOzOjPbaWaf6mX7T4bb1YbvK854bbqZPWZmjWb2cq5/UzP7k5m5mRX1d32kMBQKMuQMpg+gwVQW\n4CZgNjANOBf4FzO7KNuGZva3wPXA+cB0YCbw7xmb/Bx4AagA/g34pZlVdtnH1cBgqr/0B3fXoqXf\nFmATcEGO1y4FXgQOAE8DJ2S8dj3wClAPrAIuy3jtfcBfgK8D+4D/CNc9BfwPsB/YCFyc8Z7HgQ9m\nvL+nbWcAT4THfgT4LvDTHHU4B6gG/hXYCfwfMAa4H6gJ938/MDnc/magHWgCGoDvhOvnAg+H9VkD\nvLMffvfbgDdlPP8icFeObe8EvpTx/HxgZ/h4DtAMlGe8/iRwXcbzUcBa4HTAgaJC/+1p6Z9FLQUZ\nEGZ2MnAb8GGCb5/fB+7LOGXxCnAWwYfNvwM/NbNJGbs4DdgAjCf4oO1ctwYYB/w38CMzsxxF6Gnb\nO4G/huW6Cbiml+pMBMYSfCO/lqDF/ePw+VTgEPAdAHf/N4IP1I+5e5m7f8zMRhAEwp1hfa4CbjGz\n12U7mJndYmYHcizLw23GAMcAyzLeugzIus9wfddtJ5hZRfjaBnev72FfXwL+lyAYZRhRKMhA+RDw\nfXd/zt3bPTjf30zwTRN3/4W7b3f3Dne/G1gHnJrx/u3u/m13b3P3Q+G6ze7+A3dvB+4AJgETchw/\n67ZmNhV4PXCDu7e4+1PAfb3UpQO40d2b3f2Qu+9191+5e2P4QXozcHYP778U2OTuPw7r8zzwK+CK\nbBu7+z+6++gcywnhZmXhz9qMt9YC5TnKUJZlW8Ltu7522L7MbBFwJvDtHuooQ5RCQQbKNODTmd9y\ngSkE324xs/eY2YsZry0g+FbfaWuWfaa/pbp7Y/iwLMt2PW17DLAvY12uY2WqcfemzidmVmpm3zez\nzWZWR3AqarSZxXO8fxpwWpffxdUELZDXqiH8OTJj3UiCU2K5tu+6LeH2XV9L78vMYsAtwMfdve0o\nyiuDlEJBBspW4OYu33JL3f3nZjYN+AHwMaDC3UcDK4DMU0H5ms53BzDWzEoz1k3p5T1dy/Jp4Djg\nNHcfCfxNuN5ybL8V+HOX30WZu38k28HM7Htm1pBjWQng7vvDupyY8dYTgZU56rAyy7a73H1v+NpM\nMyvv8vpKgnBYBNxtZjuBxeHr1WZ2Vo5jyRCiUJB8SJhZScZSRPChf52ZnWaBEWb2d+EHzwiCD84a\nADN7P0FLIe/cfTOwBLjJzJJmdgbw5iPcTTlBP8IBMxsL3Njl9V0Eo3s63Q/MMbNrzCwRLq83s3k5\nynhdGBrZlszz/D8BPm9mY8xsLsEpu9tzlPknwAfMbH7YH/H5zm3dfS3BgIAbw3+/y4ATCE5x1RK0\nrk4Kl0vC/Z0CPNfzr0mGAoWC5MODBB+SnctN7r6E4EPqOwQjdNYTjArC3VcBXwWeIfgAPZ5gtNFA\nuRo4A9hLMLLpboL+jr76BpAC9gDPAn/o8vo3gSvMbL+ZfSvsd3gTcCWwneDU1n8BxRydGwk67DcD\nfwa+4u5/ADCzqWHLYipAuP6/gcfC7TdzeJhdSdAi2A98GbjC3Ws8sLNzIQxyglZGy1GWXwYBc9dN\ndkQymdndwMvu3vUbv8iwp5aCRF546maWmcXCi73eCtxb6HKJFIKuRhQJRv38muA6hWrgI+7+QmGL\nJFIYOn0kIiJpOn0kIiJpQ+700bhx43z69OmFLoaIyJCydOnSPe5e2dt2Qy4Upk+fzpIlSwpdDBGR\nIcXMNvdlO50+EhGRNIWCiIikKRRERCRNoSAiImkKBRERSVMoiIhImkJBRETSIhMKa3bW89U/rmFv\nw5HMiCwiEi2RCYUNNQ18+0/r2V2vUBARySUyoVCSCG6X29TaXuCSiIgMXpELhUMKBRGRnCITCqmk\nWgoiIr2JTCiUJIKqNrV2FLgkIiKDV2RCIdV5+qhFLQURkVyiFwo6fSQiklNkQqFYo49ERHoVmVBI\nKRRERHoVmVBIxI14zHT6SESkB5EJBTOjpCim0UciIj2ITChAcK2CWgoiIrlFKhRKEnGaNCRVRCSn\n6IVCm0JBRCSXSIVCKhHXxWsiIj2IXiioT0FEJKdIhUJxQqOPRER6EqlQSCXiunhNRKQH0QoFDUkV\nEelRpEKhpEgtBRGRnkQqFFJJjT4SEelJpEKhJBFXR7OISA/yFgpmdpuZ7TazFTlev9rMlofL02Z2\nYr7K0qkkEaOlvYP2Ds/3oUREhqR8thRuBy7q4fWNwNnufgLwReDWPJYF0PTZIiK9yVsouPsTwL4e\nXn/a3feHT58FJuerLJ1SSd19TUSkJ4OlT+EDwO9zvWhm15rZEjNbUlNT85oPUlKkloKISE8KHgpm\ndi5BKPxrrm3c/VZ3X+TuiyorK1/zsUqSCgURkZ4UFfLgZnYC8EPgYnffm+/jvdqnoBFIIiLZFKyl\nYGZTgV8D17j72oE4ZkkiqK76FEREsstbS8HMfg6cA4wzs2rgRiAB4O7fA24AKoBbzAygzd0X5as8\n8GpLQRewiYhkl7dQcPerenn9g8AH83X8bEo0JFVEpEcF72geSJ2hoNNHIiLZRSoUUhp9JCLSo0iF\nQklRUF2NPhIRyS5SoaArmkVEehapUOi8olmjj0REsotUKMRiRrIoRlObQkFEJJtIhQKE92lWS0FE\nJKtIhoL6FEREsotcKJQkYhp9JCKSQwRDQS0FEZFcIhcKqWRcF6+JiOQQuVAoKVIoiIjkErlQSCV1\n+khEJJfohUIirovXRERyiFwoFGv0kYhITpELhVRCfQoiIrlEMhTUpyAikl3kQqEkbCm4e6GLIiIy\n6EQuFFLJOB0OLe3qVxAR6SpyoZC+T3OLQkFEpKsIhkJ49zVNny0i0k3kQiGV0I12RERyiW4oaASS\niEg3kQuFdJ+CQkFEpJvIhoJaCiIi3UUuFFJJtRRERHKJXCikRx9p/iMRkW4iFwoafSQiklt0Q0Gn\nj0REuolcKBRr9JGISE6RC4WUQkFEJKe8hYKZ3WZmu81sRY7Xzcy+ZWbrzWy5mZ2cr7JkSsSNeMx0\n+khEJIt8thRuBy7q4fWLgdnhci3wv3ksS5qZUVKku6+JiGSTt1Bw9yeAfT1s8lbgJx54FhhtZpPy\nVZ5MqaRutCMikk0h+xSqgK0Zz6vDdd2Y2bVmtsTMltTU1Bz1gYuL4jRpSKqISDeFDAXLsi7r7dDc\n/VZ3X+TuiyorK4/6wKlkXFNni4hkUchQqAamZDyfDGwfiAOnEnFdvCYikkUhQ+E+4D3hKKTTgVp3\n3zEQBy5JxNSnICKSRVG+dmxmPwfOAcaZWTVwI5AAcPfvAQ8ClwDrgUbg/fkqS1cliTj1TW0DdTgR\nkSEjb6Hg7lf18roDH83X8XuSSsSpqW8uxKFFRAa1yF3RDEFLQaePRES6i2QopBJxTXMhIpJFNEMh\nqdFHIiLZRDIUihOa5kJEJJtIhkIqEaelvYP2jqzXyomIRFZkQwE0fbaISFeRDIUShYKISFaRDAXd\nklNEJLtIhkJJUi0FEZFsohkKRUG1NQJJRORwkQyFVFKnj0REsolmKHT2KegCNhGRw0QyFDT6SEQk\nu0iHgk4fiYgcLpKhkNLoIxGRrCIZChp9JCKSXSRDQaOPRESyi2QolBRp9JGISDaRDIVYzEgWxWhq\nUyiIiGSKZChAePc1tRRERA7Tp1Aws3f0Zd1QktJ9mkVEuulrS+GzfVw3ZJTo7msiIt0U9fSimV0M\nXAJUmdm3Ml4aCbTls2D5VqKWgohINz2GArAdWAK8BViasb4e+GS+CjUQUsm4Ll4TEemix1Bw92XA\nMjO7091bAcxsDDDF3fcPRAHzpaRIoSAi0lVf+xQeNrORZjYWWAb82My+lsdy5V0qqdNHIiJd9TUU\nRrl7HXA58GN3PwW4IH/Fyr9UIq6L10REuuhrKBSZ2STgncD9eSzPgCnW6CMRkW76GgpfAB4CXnH3\nxWY2E1iXv2LlXyqhPgURka56G30EgLv/AvhFxvMNwNvzVaiBoIvXRES66+sVzZPN7DdmttvMdpnZ\nr8xscr4Ll08lYUvB3QtdFBGRQaOvp49+DNwHHANUAb8L1/XIzC4yszVmtt7Mrs/y+lQze8zMXjCz\n5WZ2yZEU/mikknE6HFra1a8gItKpr6FQ6e4/dve2cLkdqOzpDWYWB74LXAzMB64ys/ldNvs8cI+7\nLwSuBG45otIfheLOG+20KBRERDr1NRT2mNm7zSweLu8G9vbynlOB9e6+wd1bgLuAt3bZxgmmzAAY\nRXAF9YBI35JT02eLiKT1NRT+gWA46k5gB3AF8P5e3lMFbM14Xh2uy3QT8G4zqwYeBP4p247M7Foz\nW2JmS2pqavpY5J6lErrRjohIV30NhS8C73X3SncfTxASN/XyHsuyrmuv7lXA7e4+mWDivf8zs25l\ncvdb3X2Ruy+qrOzxrFWflSR0S04Rka76GgonZM515O77gIW9vKcamJLxfDLdTw99ALgn3OczQAkw\nro9lOiqdLQVdqyAi8qq+hkIsnAgPgHAOpN6ucVgMzDazGWaWJOhIvq/LNluA88N9ziMIhf45P9QL\ntRRERLrr08VrwFeBp83slwSngN4J3NzTG9y9zcw+RnAldBy4zd1XmtkXgCXufh/waeAHZvbJcL/v\n8wG6cKAkEY4+UiiIiKT19Yrmn5jZEuA8gr6Cy919VR/e9yBBB3LmuhsyHq8CzjyiEveT9OgjzX8k\nIpLW15ZC5wd4r0EwVGj0kYhId33tUxh21KcgItJd5ENBfQoiIq+KbChoSKqISHeRDYVE3IiZTh+J\niGSKbCiYWXijHY0+EhHpFNlQgGBYqloKIiKvinQoFBfFadKQVBGRtEiHQioZ19TZIiIZoh0Kibgu\nXhMRyRDpUChJxNSnICKSIeKhoNFHIiKZIh0KwZBUtRRERDpFOhRKFAoiIoeJdCikErpOQUQkU7RD\nIanRRyIimSIdCsWJGE1t6mgWEekU6VBIJeK0tHXQ3jEgdwAVERn0Ih8KoOmzRUQ6RToUdKMdEZHD\nRToUUrolp4jIYSIdCiVJtRRERDJFOxSKguprqgsRkUCkQyGV1OkjEZFM0Q6Fzj4FXcAmIgJEPBQ0\n+khE5HAKBXT6SESkU8RDobOjWaEgIgIRD4VXr2jW6CMREYh6KGj0kYjIYSIdCiVFGn0kIpIpr6Fg\nZheZ2RozW29m1+fY5p1mtsrMVprZnfksT1exmJEsitHUplAQEQEoyteOzSwOfBe4EKgGFpvZfe6+\nKmOb2cBngTPdfb+Zjc9XeXJJJeI0qaUgIgLkt6VwKrDe3Te4ewtwF/DWLtt8CPiuu+8HcPfdeSxP\nViWJmPoURERC+QyFKmBrxvPqcF2mOcAcM/uLmT1rZhdl25GZXWtmS8xsSU1NTb8WMpWIa/SRiEgo\nn6FgWdZ1vcVZETAbOAe4CvihmY3u9ib3W919kbsvqqys7NdCliTiaimIiITyGQrVwJSM55OB7Vm2\n+a27t7r7RmANQUgMmJJEXBeviYiE8hkKi4HZZjbDzJLAlcB9Xba5FzgXwMzGEZxO2pDHMnWTUiiI\niKTlLRTcvQ34GPAQsBq4x91XmtkXzOwt4WYPAXvNbBXwGPD/3H1vvsqUTSqp00ciIp3yNiQVwN0f\nBB7ssu6GjMcOfCpcCqIkEdPFayIioUhf0QydfQoafSQiAgoF9SmIiGSIfChoSKqIyKsiHwqdLYWg\ne0NEJNoUCsk4HQ4t7epXEBGJfCgUF4V3X2tRKIiIRD4UOm+0o+mzRUQUCulbcupaBRERhQIlCd2S\nU0SkU+RDobOloGsVREQUCmopiIhkUCgkwtFHCgUREYVCevSR5j8SEVEodPYpvLBlv65qFpHIi3wo\nTB1bypvmT+AHT27ko3c+T11Ta6GLJCJSMJEPBTPje+8+hesvnstDK3fxlm8/xcrttYUulohIQUQ+\nFABiMeO6s2dx17Wnc6i1nctueZo7n9ui00kiEjkKhQyvnz6WB//5LE6bMZbP/eYlPnH3i9TrdJKI\nRIhCoYuKsmLueP+pfPrCOfxu2Xbe8OU/8R/3r2LL3sZCF01EJO9sqJ0iWbRokS9ZsmRAjrW8+gA/\neHIjv39pB+3unD93PO97wwzOPLYCMxuQMoiI9AczW+rui3rdTqHQu521Tfzsuc3c+dwW9h5sYfb4\nMq48dSoXzpvA1IrSAS2LiMhroVDIg6bWdh5YvoM7ntnE8upghNLs8WWcP28CF84fz0lTxhCPqQUh\nIoOPQiHPNu89yKOrd/PI6l38deM+2jqcsSOSvGn+BD541gyOHV9e6CKKiKQpFAZQ7aFWnlhbw6Or\nd/HQyl00tbVz8YKJ/OM5x7KgalShiyciolAolH0HW7jtqY3c8fQm6pvbOG/ueD567rGcMm1MoYsm\nIhGmUCiw2kOt/N8zm/jRUxvZ39jKGTMruOaMaVwwbwLJIo0EFpGBpVAYJBpb2rjzuS386KmN7Kht\nYuyIJJctrOKdi6Zw3ET1O4jIwFAoDDLtHc6T62q4Z8lWHl61i9Z258Qpo3nXoimcfVwlx4wq6dO1\nD3VNrRhQXpLIf6FFZNhQKAxiexua+c0L27hnyVbW7moAoLK8mJOmjE4vJ0weRWu7s2JbLSu217Jy\nWx0rtteyeW8jpck4/3jOLD541sz0neN6OtZDK3fR1tFBUSxGUdxIxI1EPEYyHuP0WRWMVMCIDHsK\nhSHA3Vm1o46lm/fz4pYDvLj1ABv2HMy67dSxpSyoGsnrjhnFS9W1/GHlTqpGp7j+4rlcesKkbq2M\njXsO8sMnN/DLpdU0t+W+gdCEkcX85+XHc97cCf1aNxEZXBQKQ1RtYyvLqg+wvPoAyaIYC6pG8bpJ\noxhVevi3+Wde2csX71/Fqh11nDJtDDdcOp8Tp4xm6eZ93PrEBv64aheJWIzLT67ivW+YTmV5MW3t\nTmt7B63tHbR1OLvrmvni/atYs6uey0+u4sZLX9ftOJ0ONrfxwEs7ONjcxlWnTu21hSIig8ugCAUz\nuwj4JhAHfujuX86x3RXAL4DXu3uPn/jDPRSORHuH88ulW/nKQ2vY09DCrMoRvFJzkNGlCa45fRrX\nnDGN8eUlPe6jpa2D7/xpHd99/BUqRiT50mXHc8H8oNXg7jy/5QD3LN7K/cu3c7AluI/1lLEpbnrz\n6zh/nloXIkNFwUPBzOLAWuBCoBpYDFzl7qu6bFcOPAAkgY8pFI5cfVMr333sFZ7buJe3nVTFOxZN\npjRZdET7WLGtls/8Yhkv76zn8oVVzJs0kruXbGX97gZKk3H+7vhJvOv1U2hp7+CG365k/e4GLpw/\ngRvfPJ/JYzT/k8hgNxhC4QzgJnf/2/D5ZwHc/T+7bPcN4BHgM8BnFAqF09LWwXceW88tj62nrcNZ\nODUYHXXpicdQVlx02Ha3/WUj33xkHY7zT+fN5kNnzdT1FyKDWF9D4ci+Th6ZKmBrxvNq4LTMDcxs\nITDF3e83s8/k2pGZXQtcCzB16tQ8FFUAkkUxPnXhHK44eTKtHR3MqizLud11Z8/izScewxd+t5Kv\nPLSGuxdv5by541k0fQyLpo1l4qjup63cnV11zazeUcfLO+sZmSpi/qSRzJ04klRSfRQig0E+QyHb\noPt0s8TMYsDXgff1tiN3vxW4FYKWQj+VT3Lo63TgVaNTfP+aRTz28m5+8OQG7l68lduf3gTA5DEp\nFk0bw/GTR7Oz9hCrdtSxekc9+w62dNtPzGBmZRnzJ41k/jEjmTCymPYO6Ohw2jqcdnc6Opx4zBg7\nIsnYEUkqwp+jS5P9MjPtoZZ2fr9iB+t2N1BcFCNZFKO4KJ5+PGFkCWcdO46YZsGVYS6foVANTMl4\nPhnYnvG8HFgAPB4Op5wI3Gdmb+ntFJIMLufOHc+5c8fT2t7B6h11LN60n6Wb9/GXV/Zy74vbSRbF\nmDuxnAvnTWD+McEH/3ETy6ltbGXVjjpWba9LD829b9n23g+YIWYwpjRJRVmScWXFVJQVMy58XFlW\nzHETy5k7qZziouwtkdU76rjrr1v49QvbqG9qIx4z2juyf++YO7GcT1wwh7993YQeLzTcXdfEQ6t2\ngTsVZcVUjAjKVzGimFGpxJAPFndnw56DTB1bSiKuU4bDTT77FIoIOprPB7YRdDT/vbuvzLH946hP\nYVhxd2oamhlbmqSojx8eBxpb2N/YSlHMiMWMuBmxGMTNaOtw9h1sYd/BFvYebGFfQ3P68Z6GZvY0\ntLA3/NnQ3JbeZzIeY96kck6YHFwUuKBqFMurD/Dzv27lxa3B0N9LFkzkylOnctqMsbhDS3sHzW0d\nNLe109LWwdLN+/nmI+vYsOcgC6pG8qkL53DucePT4dDU2s5DK3fy6+e38eS6GnLkCvGYMatyBKfP\nrOCMmRWcOmMsFWXF3bZra+9g095G1u6qp6a+mWPHlzF3YnnWbQdKW3sHD7y0g+/9eQOrd9QxraKU\nT1wwm7ecWKX7iAwBBe9oDgtxCfANgiGpt7n7zWb2BWCJu9/XZdvHUShIPznU0s6uuiZW7ahjWfUB\nlm09wIptdYeFRecd9C5fWMWYEcle99nW3sG9L27nW4+uY8u+Rk6aMpprTp/G4k37eGD5Duqb26ga\nneKyhVW8beExjEwlgtBqCEKr8+eK7XUs2bSPxnCI73ETyjl95ljGjyxh3a561uxq4JXdDbS0d7/o\nsLK8mLkTy5k3aSTHji8jGY8Fp9g6OsKfwTKtopSFU8b0qV69aWxp457FW/nBkxvZduAQsypHcMUp\nU/jdsu2s2lHH7PFlfOrCOVy0YKJuUzuIDYpQyAeFgrxWHR3Ohj0NvLStliljSjll2pjX9CHW2t7B\nr5ZW8+0/rWfbgUOUJuNccvwkLj+5itNnVPTp9FBrewfLq2t5dsNent2wlyWb9nOotZ1jRpUwZ2I5\nx00oZ86Eco6bWM64smJeqWlId9C/vLOOtbsaaOnhSvVOMytHcPLUMZwybQwnTx3D+PJiGprbqGtq\npaGpjYbmYGlqbU+/xzq7Aw227mvkp89uZn9jK4umjeHDZ8/i/LnjicWMjg7n9yt28rWH1/BKTdCC\n+vSbjuOcOZV9/r2+sGU/SzfvZ86EchZUjWJsP4TYULa7vonfPL+Nt58ymXH93CpUKIjkWXNbO8u2\n1rKgauQRXxfSVWt7B02t7X2e6LCtvYNtBw7R4VAUM+IxS/8EWLe7gaWb96c/dPc3tr7msl0wbwLX\nnT2TRdPHZn29vcO594VtfOPRtWzdd4gFVSN5/xtmcOmJk3L25bxUXcvXHl7DY2tqDltfNTrFgqqR\nLDhmFLMnlNHU2kHtoVbqDrVSGy4NzW1UlCWZNnYEUytKmVZRytSxpUf9b3A03P2oWkmt7R385JnN\nfOPhtdQ3tzF/0kju+vDp/TovmUJBRIDgA2vT3kaWbt5P3aFWykuKKC9JUF5SRFlxEWUlRZQk4hiv\nDg/s/FxIJeJ97sdoaevgV89X86OnNrJ+dwPjyoq5+rSpXH361PSV9au21/H1R9by8KpdjC5NcO3f\nzOSyhVVsrDnIS9tqWbG9jhXbatmYZQ6w0mScUakEI4qLqKlvpvbQ4UFXWV7McWGL4/hwmTI2dVQf\n1nVNrWzac5CNew6yaU8jm/cdZP/BFurDVlZ9U9jqam5j9vgyPnTWTN56UtURXbPz9Ct7uOm+lazd\n1cDfzKnk4gUTueG3K1g4ZQw/+cCp/TaljEJBRArC3Xly3R5+/JeNPLamhkTcePMJx9DcFnRUl5cU\n8aGzZvL+M6fnbBnVN7WyaU8jpcVBEIwsSXT7oD3Q2MLmvY1s3tfIlr0H2bS3kZd31rFmZz2t7cHn\n2qhUguOrRjG1opRkPEZRzEgUxUjEjKJ4DAMaW9s52NzGweZ2GlvaONjSTt2hVrbsa+w2hHriyBLG\nlScpL04cFq4jiuM8uno3L++sZ+LIEj7wxhlcddrUwy767GpH7SFufmA19y/fweQxKW64dD4Xzg9G\ntv1u2Xb++a4XOO+48XzvmlP6ZZSXQkFECm7jnoPc8fQmfrFkK2bGP5w5nQ+8cWbOiRf7Q3NbO2t3\nBn1HL22rZcW2WnbUHqI1nBCyrd0P68RPxmOUFscZkSyiNBmntLiIsuI4U8aUMn3cCKZXjGDGuBFM\nHVva40WW7s6f19bw/T9v4JkNeykvKeKa06dx4fwJ1NQ3s6O2ie21h9hxoIkdtYdYsa2ODnc+cs4s\nrjt7VrcWwU+f3czn713BZQur+Oo7TjzqocwKBREZNBpbglFfhTzvn8k9GKXlkJdrLV7ceoBbn3iF\n36/YSeZHbCJuTBxVwjGjUswaX8ZHzp7FlLG5Lxb9zp/W8T9/XMv73jCdG988/6hOhQ2GaS5ERIDB\nEwadzIyieP6Gz540ZTS3XH0Km/YcZM2u4JTSpNEljBtRfETf+D967rHsb2zlR09tZExpko9fMDtv\nZe40uP6lRESGkenjRjB93IjX/H4z498umceBxla+/shaxoxI8J4zpvdfAbNQKIiIDGKxmPFfbz+e\n1vYOJo9J5f14CgURkUGuKB7jW1ctHJBjaTYrERFJUyiIiEiaQkFERNIUCiIikqZQEBGRNIWCiIik\nKRRERCRNoSAiImlDbkI8M6sBNr/Gt48D9vRjcYaSqNZd9Y4W1Tu3ae5e2duOhlwoHA0zW9KXWQKH\no6jWXfXfhdTHAAAHVUlEQVSOFtX76On0kYiIpCkUREQkLWqhcGuhC1BAUa276h0tqvdRilSfgoiI\n9CxqLQUREemBQkFERNIiEwpmdpGZrTGz9WZ2faHLky9mdpuZ7TazFRnrxprZw2a2Lvw5ppBlzAcz\nm2Jmj5nZajNbaWYfD9cP67qbWYmZ/dXMloX1/vdw/Qwzey6s991mlix0WfPBzOJm9oKZ3R8+H/b1\nNrNNZvaSmb1oZkvCdf32dx6JUDCzOPBd4GJgPnCVmc0vbKny5nbgoi7rrgcedffZwKPh8+GmDfi0\nu88DTgc+Gv4bD/e6NwPnufuJwEnARWZ2OvBfwNfDeu8HPlDAMubTx4HVGc+jUu9z3f2kjGsT+u3v\nPBKhAJwKrHf3De7eAtwFvLXAZcoLd38C2Ndl9VuBO8LHdwBvG9BCDQB33+Huz4eP6wk+KKoY5nX3\nQEP4NBEuDpwH/DJcP+zqDWBmk4G/A34YPjciUO8c+u3vPCqhUAVszXheHa6LignuvgOCD09gfIHL\nk1dmNh1YCDxHBOoenkJ5EdgNPAy8Ahxw97Zwk+H69/4N4F+AjvB5BdGotwN/NLOlZnZtuK7f/s6L\n+qGAQ4FlWaexuMOQmZUBvwI+4e51wZfH4c3d24GTzGw08BtgXrbNBrZU+WVmlwK73X2pmZ3TuTrL\npsOq3qEz3X27mY0HHjazl/tz51FpKVQDUzKeTwa2F6gshbDLzCYBhD93F7g8eWFmCYJA+Jm7/zpc\nHYm6A7j7AeBxgj6V0WbW+aVvOP69nwm8xcw2EZwOPo+g5TDc6427bw9/7ib4EnAq/fh3HpVQWAzM\nDkcmJIErgfsKXKaBdB/w3vDxe4HfFrAseRGeT/4RsNrdv5bx0rCuu5lVhi0EzCwFXEDQn/IYcEW4\n2bCrt7t/1t0nu/t0gv/Pf3L3qxnm9TazEWZW3vkYeBOwgn78O4/MFc1mdgnBN4k4cJu731zgIuWF\nmf0cOIdgKt1dwI3AvcA9wFRgC/AOd+/aGT2kmdkbgSeBl3j1HPPnCPoVhm3dzewEgo7FOMGXvHvc\n/QtmNpPgG/RY4AXg3e7eXLiS5k94+ugz7n7pcK93WL/fhE+LgDvd/WYzq6Cf/s4jEwoiItK7qJw+\nEhGRPlAoiIhImkJBRETSFAoiIpKmUBARkTSFggwaZvZ0+HO6mf19P+/7c9mOlS9m9jYzuyFP+/5c\n71sd8T6PN7Pb+3u/MvRoSKoMOpnjzo/gPfFwuodcrze4e1l/lK+P5XkaeIu77znK/XSrV77qYmaP\nAP/g7lv6e98ydKilIIOGmXXO9vll4KxwvvhPhhO+fcXMFpvZcjP7cLj9OeE9FO4kuGgNM7s3nChs\nZedkYWb2ZSAV7u9nmceywFfMbEU4R/27Mvb9uJn90sxeNrOfhVdNY2ZfNrNVYVn+J0s95gDNnYFg\nZreb2ffM7EkzWxvO29M5kV2f6pWx72x1ebcF91R40cy+H04Vj5k1mNnNFtxr4VkzmxCuf0dY32Vm\n9kTG7n9HcHWwRJm7a9EyKBagIfx5DnB/xvprgc+Hj4uBJcCMcLuDwIyMbceGP1MEl/9XZO47y7He\nTjCzaByYQHA16KRw37UE8+fEgGeANxJcKbuGV1vZo7PU4/3AVzOe3w78IdzPbIK5uEqOpF7Zyh4+\nnkfwYZ4In98CvCd87MCbw8f/nXGsl4CqruUnmE/od4X+O9BS2CUqs6TK0PYm4AQz65zTZhTBh2sL\n8Fd335ix7T+b2WXh4ynhdnt72PcbgZ97cIpml5n9GXg9UBfuuxrAgqmppwPPAk3AD83sAeD+LPuc\nBNR0WXePu3cA68xsAzD3COuVy/nAKcDisCGT4tXJ0FoyyrcUuDB8/BfgdjO7B/j1q7tiN3BMH44p\nw5hCQYYCA/7J3R86bGXQ93Cwy/MLgDPcvdHMHif4Rt7bvnPJnDOnHShy9zYzO5Xgw/hK4GMEM3Rm\nOkTwAZ+pa+ed08d69cKAO9z9s1lea3X3zuO2E/5/d/frzOw0ghvUvGhmJ7n7XoLf1aE+HleGKfUp\nyGBUD5RnPH8I+IgFU2NjZnPCGSK7GgXsDwNhLsEU0p1aO9/fxRPAu8Lz+5XA3wB/zVUwC+7XMMrd\nHwQ+QXALzK5WA8d2WfcOM4uZ2SxgJsEpqL7Wq6vMujwKXGHB3Pqd9+qd1tObzWyWuz/n7jcAe3h1\nWvk5BKfcJMLUUpDBaDnQZmbLCM7Hf5Pg1M3zYWdvDdlvN/gH4DozW07woftsxmu3AsvN7HkPplju\n9BvgDGAZwbf3f3H3nWGoZFMO/NbMSgi+pX8yyzZPAF81M8v4pr4G+DNBv8V17t5kZj/sY726Oqwu\nZvZ5gjtxxYBW4KPA5h7e/xUzmx2W/9Gw7gDnAg/04fgyjGlIqkgemNk3CTptHwnH/9/v7r/s5W0F\nY2bFBKH1Rn/1dpYSQTp9JJIfXwJKC12IIzAVuF6BIGopiIhImloKIiKSplAQEZE0hYKIiKQpFERE\nJE2hICIiaf8fD9Z3e5dnLmEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1cf265c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(training_set, Y_train, layers_dims, num_iterations = 50000, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    A2, cache = L_model_forward(X, parameters)\n",
    "    predictions = np.array([0 if i <= 0.5 else 1 for i in np.squeeze(A2)])\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85%\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(parameters,training_set)\n",
    "print ('Accuracy: %d' % float((np.dot(Y_train,pred_train.T) + np.dot(1-Y_train,1-pred_train.T))/float(Y_train.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83%\n"
     ]
    }
   ],
   "source": [
    "pred_dev = predict(parameters, dev_set)\n",
    "print ('Accuracy: %d' % float((np.dot(Y_dev,pred_dev.T) + np.dot(1-Y_dev,1-pred_dev.T))/float(Y_dev.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_array = test_df.as_matrix()\n",
    "test_array = np.transpose(test_array)\n",
    "Y_test_predictions = predict(parameters, test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test_predictions = Y_test_predictions.astype(int)\n",
    "Y_test_df = pd.DataFrame(np.transpose(Y_test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df_copy['Survived'] = Y_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "labels ['Pclass' 'Name' 'Sex' 'Age' 'SibSp' 'Parch' 'Ticket' 'Fare' 'Cabin'\n 'Embarked'] not contained in axis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-337-d2b6a32495cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_df_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df_copy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Pclass'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Sex'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Age'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'SibSp'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Parch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Ticket'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Fare'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Cabin'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Embarked'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, level, inplace, errors)\u001b[0m\n\u001b[1;32m   2159\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2161\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2162\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   3622\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3623\u001b[0m                 raise ValueError('labels %s not contained in axis' %\n\u001b[0;32m-> 3624\u001b[0;31m                                  labels[mask])\n\u001b[0m\u001b[1;32m   3625\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3626\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: labels ['Pclass' 'Name' 'Sex' 'Age' 'SibSp' 'Parch' 'Ticket' 'Fare' 'Cabin'\n 'Embarked'] not contained in axis"
     ]
    }
   ],
   "source": [
    "test_df_copy = test_df_copy.drop(['Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare', 'Cabin','Embarked'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>897</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>898</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         0\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         1\n",
       "5          897         0\n",
       "6          898         0\n",
       "7          899         0\n",
       "8          900         1\n",
       "9          901         0"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_copy.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df_copy.to_csv('predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
