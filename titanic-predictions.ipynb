{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dnn_utils_v2 import *\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "test_df_copy = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preview data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combine the data from train and test to not perform the same data clean up operations twice - once per data set\n",
    "targets = train_df.Survived\n",
    "train_df.drop('Survived', 1, inplace=True)\n",
    "combined_sets = train_df.append(test_df)\n",
    "combined_sets.reset_index(inplace=True)\n",
    "combined_sets.drop('index', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1309, 11)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_sets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropping PassengerId as it's just an index. \n",
    "combined_sets = combined_sets.drop(['PassengerId'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass         0\n",
       "Name           0\n",
       "Sex            0\n",
       "Age          263\n",
       "SibSp          0\n",
       "Parch          0\n",
       "Ticket         0\n",
       "Fare           1\n",
       "Cabin       1014\n",
       "Embarked       2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for missing values in the combined set. \n",
    "combined_sets.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dummy encoding Pclass as it is also categorical\n",
    "pclass_dummies = pd.get_dummies(combined_sets['Pclass'], prefix=\"Pclass\")\n",
    "combined_sets = pd.concat([combined_sets,pclass_dummies],axis=1)\n",
    "combined_sets.drop('Pclass',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dummy encoding Sex column\n",
    "gender_dummies = pd.get_dummies(combined_sets['Sex'],prefix='Sex')\n",
    "combined_sets = pd.concat([combined_sets,gender_dummies],axis=1)\n",
    "combined_sets.drop('Sex',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extracting a column with a title\n",
    "combined_sets['Title'] = combined_sets['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dummy encoding titles\n",
    "title_dummies = pd.get_dummies(combined_sets['Title'], prefix=\"Title\")\n",
    "combined_sets = pd.concat([combined_sets,title_dummies],axis=1)\n",
    "combined_sets.drop('Title',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extracting last name\n",
    "combined_sets['Last_Name'] = combined_sets['Name'].map(lambda name:name.split(',')[0])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we can drop Name column now\n",
    "combined_sets = combined_sets.drop(['Name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adding new feature - family size inlcuding the passenger\n",
    "combined_sets['FamilySize'] = combined_sets['SibSp'] + combined_sets['Parch'] +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extracting ticket_number\n",
    "combined_sets['Ticket_Number'] = combined_sets['Ticket'].map(lambda x:x.rsplit(' ', 1)[-1])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extracting ticket prefix\n",
    "def cleanTicket(ticket):\n",
    "        ticket = ticket.replace('.','')\n",
    "        ticket = ticket.replace('/','')\n",
    "        ticket = ticket.split()\n",
    "        if len(ticket) > 1:\n",
    "            return ticket[0]\n",
    "        else:\n",
    "            return 'XXX'\n",
    "    \n",
    "combined_sets['Ticket'] = combined_sets['Ticket'].map(cleanTicket)\n",
    "ticket_dummies = pd.get_dummies(combined_sets['Ticket'], prefix='Ticket')\n",
    "combined_sets = pd.concat([combined_sets, ticket_dummies], axis=1)\n",
    "combined_sets.drop('Ticket', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C    7\n",
       "S    5\n",
       "Name: Embarked, dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combined_sets[(combined_sets['Embarked'].isnull())]['Ticket_Number'] # 113572\n",
    "combined_sets[(combined_sets['Ticket_Number'].map(lambda x: x.startswith('1135')))]['Embarked'].value_counts()\n",
    "# judging by other people with similar ticket it's equally possible that they embarked in C or S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'C' value is more common for Embarked among people with a similar ticket number\n",
    "combined_sets['Embarked'].fillna('C', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now we can dummy encode 'Embarked'\n",
    "embarked_dummies = pd.get_dummies(combined_sets['Embarked'],prefix='Embarked')\n",
    "combined_sets = pd.concat([combined_sets,embarked_dummies],axis=1)\n",
    "combined_sets.drop('Embarked',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>...</th>\n",
       "      <th>Ticket_STONO</th>\n",
       "      <th>Ticket_STONO2</th>\n",
       "      <th>Ticket_STONOQ</th>\n",
       "      <th>Ticket_SWPP</th>\n",
       "      <th>Ticket_WC</th>\n",
       "      <th>Ticket_WEP</th>\n",
       "      <th>Ticket_XXX</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>60.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age  SibSp  Parch  Fare Cabin  Pclass_1  Pclass_2  Pclass_3  \\\n",
       "1043  60.5      0      0   NaN   NaN         0         0         1   \n",
       "\n",
       "      Sex_female  Sex_male     ...      Ticket_STONO  Ticket_STONO2  \\\n",
       "1043           0         1     ...                 0              0   \n",
       "\n",
       "      Ticket_STONOQ  Ticket_SWPP  Ticket_WC  Ticket_WEP  Ticket_XXX  \\\n",
       "1043              0            0          0           0           1   \n",
       "\n",
       "      Embarked_C  Embarked_Q  Embarked_S  \n",
       "1043           0           0           1  \n",
       "\n",
       "[1 rows x 70 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learning more about the passenger with missing Fare\n",
    "combined_sets[combined_sets['Fare'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The passenger with missing fare has Pclass_3==1 Embarked_S== 1 FamilySize==1 Ticket_XXX==1 Title_Mr==1\n",
    "# Let's average fare among those with the same values\n",
    "m_fare = combined_sets[(combined_sets['Embarked_S']==1) & (combined_sets['Pclass_3']==1) & (combined_sets['FamilySize']==1) & (combined_sets['Ticket_XXX']==1) & (combined_sets['Title_Mr']==1) & (combined_sets['Age'] > 50)]['Fare'].mean()\n",
    "combined_sets['Fare'].fillna(m_fare, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dealing with NaN in Age\n",
    "def replace_age_with_mean(df, pclass, title): \n",
    "    mask = ((df[pclass]==1) & (df[title]==1))\n",
    "    med = df.loc[mask, 'Age'].mean()\n",
    "    df.loc[mask, 'Age'] = df.loc[mask, 'Age'].fillna(med)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = ['Title_Mrs','Title_Miss','Title_Mr','Title_Dr', 'Title_Master', 'Title_Ms']\n",
    "pclass = ['Pclass_1', 'Pclass_2', 'Pclass_3']\n",
    "for p in pclass:\n",
    "    for title in titles:\n",
    "        combined_sets = replace_age_with_mean(combined_sets, p, title) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# After this there's still one missing value for NaN because there was only one Ms in third class, we'll use the average among all Ms\n",
    "m = combined_sets.loc[(combined_sets['Title_Ms']==1), 'Age'].mean()\n",
    "combined_sets['Age'].fillna(m, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# time to deal with Cabin NaNs:\n",
    "# turns out here are the proportions\n",
    "# In third class there are 693 NaN out of 709\n",
    "# In second class - 254 out of 277\n",
    "# In first class - 67 out of 323\n",
    "# Intuition - the passengers in third and second class were not assigned cabins, and the ones in the first class are genuine missing values\n",
    "last_names = combined_sets[(combined_sets['Pclass_1']==1)]['Last_Name'].value_counts()\n",
    "# the list of last names that occur more than once. these might indicate a family together with the passenger class\n",
    "fams = last_names[last_names>1].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# looking for families by last name in the first class \n",
    "# list of families where some Cabin values are NaN\n",
    "fams_with_a_nan = [] \n",
    "# list of families where all Cabin values are NaN\n",
    "fams_with_all_nan = []\n",
    "for name in fams:\n",
    "    if combined_sets[(combined_sets['Pclass_1']==1) & (combined_sets['Last_Name']==name)]['Cabin'].isnull().all()==True: \n",
    "        fams_with_all_nan.append(name)\n",
    "    else:\n",
    "        if combined_sets[(combined_sets['Pclass_1']==1) & (combined_sets['Last_Name']==name)]['Cabin'].isnull().any()== True: \n",
    "            fams_with_a_nan.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# those whose family has a cabin, get assigned the same cabin\n",
    "for name in fams_with_a_nan:\n",
    "    mask = ((combined_sets['Pclass_1']==1) & (combined_sets['Last_Name']==name))\n",
    "    cabin = combined_sets[mask]['Cabin'].value_counts()\n",
    "    cabin = cabin.index.tolist()[0]\n",
    "    combined_sets.loc[mask, 'Cabin'] = combined_sets.loc[mask, 'Cabin'].fillna(cabin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now that we can't guess the exact cabin, let's replace the Cabin number with just the first letter \n",
    "mask = ((combined_sets['Cabin'].isnull()==False))\n",
    "combined_sets.loc[mask, 'Cabin'] = combined_sets.loc[mask, 'Cabin'].map(lambda c : c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the ones that don't have family members with a known cabin, we'll check what fare they have and compare to similar fares\n",
    "for name in fams_with_all_nan:\n",
    "    fare = combined_sets[(combined_sets['Pclass_1']==1) & (combined_sets['Last_Name']==name)]['Fare'].mean()\n",
    "    mask1 = ((combined_sets['Pclass_1']==1) & (combined_sets['Fare']>(fare-1)) & (combined_sets['Fare']<(fare+1)))\n",
    "    cabin = combined_sets[mask1]['Cabin'].value_counts()\n",
    "    cabin = cabin.index.tolist()\n",
    "    if not cabin: \n",
    "        cabin = np.nan\n",
    "    else: \n",
    "        cabin = cabin[0]\n",
    "    mask2 = ((combined_sets['Pclass_1']==1) & (combined_sets['Last_Name']==name))\n",
    "    combined_sets.loc[mask2, 'Cabin'] = combined_sets.loc[mask2, 'Cabin'].fillna(cabin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 52 NaN left in Cabin in first class. these are singles\n",
    "# get all list of fares for NaNs\n",
    "fares = combined_sets[(combined_sets['Pclass_1']==1) & (combined_sets['Cabin'].isnull())]['Fare'].value_counts()\n",
    "fares = fares.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for fare in fares:\n",
    "    mask1 = ((combined_sets['Pclass_1']==1) & (combined_sets['Fare']>(fare-1)) & (combined_sets['Fare']<(fare+1)))\n",
    "    cabin = combined_sets[mask1]['Cabin'].value_counts()\n",
    "    cabin = cabin.index.tolist()\n",
    "    if not cabin: \n",
    "        cabin = np.nan\n",
    "    else: \n",
    "        cabin = cabin[0]\n",
    "    mask2 = ((combined_sets['Pclass_1']==1) & (combined_sets['Fare']==fare))\n",
    "    combined_sets.loc[mask2, 'Cabin'] = combined_sets.loc[mask2, 'Cabin'].fillna(cabin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C    122\n",
       "B     74\n",
       "E     55\n",
       "D     46\n",
       "A     22\n",
       "T      1\n",
       "Name: Cabin, dtype: int64"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#still 3 left I'll assign the most popular value in the first class to them, which is C\n",
    "combined_sets[(combined_sets['Pclass_1']==1)]['Cabin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = ((combined_sets['Pclass_1']==1))\n",
    "combined_sets.loc[mask, 'Cabin'] = combined_sets.loc[mask, 'Cabin'].fillna('C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the rest Cabin values are from the second and third class, and they can be \"U\" for Unknown\n",
    "combined_sets['Cabin'].fillna('U', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now we can use dummy encoding again\n",
    "cabin_dummies = pd.get_dummies(combined_sets['Cabin'], prefix='Cabin')\n",
    "combined_sets = pd.concat([combined_sets,cabin_dummies], axis=1)\n",
    "combined_sets.drop('Cabin', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                   False\n",
       "SibSp                 False\n",
       "Parch                 False\n",
       "Fare                  False\n",
       "Pclass_1              False\n",
       "Pclass_2              False\n",
       "Pclass_3              False\n",
       "Sex_female            False\n",
       "Sex_male              False\n",
       "Title_Capt            False\n",
       "Title_Col             False\n",
       "Title_Don             False\n",
       "Title_Dona            False\n",
       "Title_Dr              False\n",
       "Title_Jonkheer        False\n",
       "Title_Lady            False\n",
       "Title_Major           False\n",
       "Title_Master          False\n",
       "Title_Miss            False\n",
       "Title_Mlle            False\n",
       "Title_Mme             False\n",
       "Title_Mr              False\n",
       "Title_Mrs             False\n",
       "Title_Ms              False\n",
       "Title_Rev             False\n",
       "Title_Sir             False\n",
       "Title_the Countess    False\n",
       "Last_Name             False\n",
       "FamilySize            False\n",
       "Ticket_Number         False\n",
       "                      ...  \n",
       "Ticket_SCA4           False\n",
       "Ticket_SCAH           False\n",
       "Ticket_SCOW           False\n",
       "Ticket_SCPARIS        False\n",
       "Ticket_SCParis        False\n",
       "Ticket_SOC            False\n",
       "Ticket_SOP            False\n",
       "Ticket_SOPP           False\n",
       "Ticket_SOTONO2        False\n",
       "Ticket_SOTONOQ        False\n",
       "Ticket_SP             False\n",
       "Ticket_STONO          False\n",
       "Ticket_STONO2         False\n",
       "Ticket_STONOQ         False\n",
       "Ticket_SWPP           False\n",
       "Ticket_WC             False\n",
       "Ticket_WEP            False\n",
       "Ticket_XXX            False\n",
       "Embarked_C            False\n",
       "Embarked_Q            False\n",
       "Embarked_S            False\n",
       "Cabin_A               False\n",
       "Cabin_B               False\n",
       "Cabin_C               False\n",
       "Cabin_D               False\n",
       "Cabin_E               False\n",
       "Cabin_F               False\n",
       "Cabin_G               False\n",
       "Cabin_T               False\n",
       "Cabin_U               False\n",
       "Length: 78, dtype: bool"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_sets.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# probably can drop the Last Name now\n",
    "combined_sets = combined_sets.drop(['Last_Name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_sets = combined_sets.drop(['Ticket_Number'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#normalyzing data\n",
    "combined_sets_norm = (combined_sets - combined_sets.mean()) / (combined_sets.max() - combined_sets.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# time to split it back to the train and test sets\n",
    "train_df = combined_sets_norm.head(891)\n",
    "train_Y = targets\n",
    "test_df = combined_sets_norm.iloc[891:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891,)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting the training data into a matrix\n",
    "train_array = train_df.as_matrix()\n",
    "Y_array = train_Y.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_array = Y_array.reshape(Y_array.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splitting the training data into training set and dev set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_set, dev_set, Y_train, Y_dev = train_test_split(train_array, Y_array)\n",
    "training_set = np.transpose(training_set)\n",
    "dev_set = np.transpose(dev_set)\n",
    "Y_train = np.transpose(Y_train)\n",
    "Y_dev = np.transpose(Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 223)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking shapes\n",
    "dev_set.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    The linear part of a layer's forward propagation.\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    Z = np.add(np.matmul(W, A), b)\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Forward propagation for the LINEAR->ACTIVATION layer\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2            \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)    \n",
    "    \n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"    \n",
    "    m = Y.shape[1]\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    \n",
    "    cost = np.squeeze(cost)      \n",
    "    assert(cost.shape == ())    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost_with_regularization(AL, Y, parameters, lambd):\n",
    "    \"\"\"\n",
    "    Cost function with L2 regularization. \n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "    parameters -- python dictionary containing parameters of the model\n",
    "    lambd - regularization parameter\n",
    "    \n",
    "    Returns:\n",
    "    cost - value of the regularized loss function\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    cross_entropy_cost = compute_cost(AL, Y) #the cross-entropy part of the cost    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    W_squared_sum = 0\n",
    "\n",
    "    for l in range(L):\n",
    "        W_squared_sum = W_squared_sum + np.sum(np.square(parameters[\"W\" + str(l+1)]))\n",
    "    \n",
    "    L2_regularization_cost = W_squared_sum*(1/m)*(lambd/2)\n",
    "    \n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward_with_regularization(dZ, cache, lambd):\n",
    "    \"\"\"\n",
    "    The linear portion of backward propagation for a single layer (layer l) with regularization\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1.0/m) * np.matmul(dZ, A_prev.T) + lambd/m*W\n",
    "    db = (1.0/m) * np.sum(dZ, axis=-1, keepdims=True)\n",
    "    dA_prev = np.matmul(np.transpose(W), dZ)    \n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward_with_regularization(dA, cache, lambd, activation):\n",
    "    \"\"\"\n",
    "    The backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward_with_regularization(dZ, linear_cache, lambd)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward_with_regularization(dZ, linear_cache, lambd)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches,lambd):\n",
    "    \"\"\"\n",
    "    The backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) \n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward_with_regularization(dAL, current_cache, lambd, activation = 'sigmoid')\n",
    "    \n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward_with_regularization(grads[\"dA\"+str(l+2)],current_cache,lambd,\"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp     \n",
    "                \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing updated parameters \n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting the number of layers in NN and their sizes\n",
    "layers_dims = [training_set.shape[0], 20, 10, 10, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.006, num_iterations = 3000, lambd = 0, print_cost=False):\n",
    "    \"\"\"\n",
    "    A L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    lambd - regularization parameter\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    costs = []                         # keep track of cost\n",
    "    m = X.shape[1]                        # number of examples\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)    \n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Cost function\n",
    "        cost = compute_cost_with_regularization(AL, Y, parameters, lambd)\n",
    "            \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches, lambd)\n",
    " \n",
    "        # Update parameters. (without optimizations)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "                \n",
    "        # Print the cost every 1000 training example\n",
    "        if print_cost and i % 10000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 10000 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.718082\n",
      "Cost after iteration 10000: 0.358144\n",
      "Cost after iteration 20000: 0.309601\n",
      "Cost after iteration 30000: 0.276863\n",
      "Cost after iteration 40000: 0.258832\n",
      "Cost after iteration 50000: 0.246310\n",
      "Cost after iteration 60000: 0.233664\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1ca85a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(training_set, Y_train, layers_dims, num_iterations = 70000, lambd = 0.7, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    A2, cache = L_model_forward(X, parameters)\n",
    "    predictions = np.array([0 if i <= 0.5 else 1 for i in np.squeeze(A2)])\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91%\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(parameters,training_set)\n",
    "print ('Accuracy: %d' % float((np.dot(Y_train,pred_train.T) + np.dot(1-Y_train,1-pred_train.T))/float(Y_train.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80%\n"
     ]
    }
   ],
   "source": [
    "pred_dev = predict(parameters, dev_set)\n",
    "print ('Accuracy: %d' % float((np.dot(Y_dev,pred_dev.T) + np.dot(1-Y_dev,1-pred_dev.T))/float(Y_dev.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_array = test_df.as_matrix()\n",
    "test_array = np.transpose(test_array)\n",
    "Y_test_predictions = predict(parameters, test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test_predictions = Y_test_predictions.astype(int)\n",
    "Y_test_df = pd.DataFrame(np.transpose(Y_test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df_copy['Survived'] = Y_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_copy = test_df_copy.drop(['Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare', 'Cabin','Embarked'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         1\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         1"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df_copy.to_csv('predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
